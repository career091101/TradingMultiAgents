#!/usr/bin/env python3
"""
ã‚·ãƒ³ãƒ—ãƒ«ãªOpenAI APIç–é€šãƒ†ã‚¹ãƒˆ
- åŸºæœ¬çš„ãªæ¥ç¶šç¢ºèª
- ãƒ¢ãƒ‡ãƒ«åˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç¢ºèª
"""

import os
import asyncio
import json
import time
from datetime import datetime
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def test_openai_connection():
    """OpenAI APIæ¥ç¶šãƒ†ã‚¹ãƒˆï¼ˆç´”ç²‹ãªHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆç‰ˆï¼‰"""
    
    logger.info("=" * 60)
    logger.info("ğŸ”‘ OpenAI API ç–é€šãƒ†ã‚¹ãƒˆ")
    logger.info("=" * 60)
    
    # APIã‚­ãƒ¼ç¢ºèª
    api_key = os.environ.get('OPENAI_API_KEY')
    if not api_key:
        logger.error("âŒ OPENAI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        logger.error("è¨­å®šæ–¹æ³•: export OPENAI_API_KEY='your-api-key-here'")
        return
        
    masked_key = f"{api_key[:8]}...{api_key[-4:]}"
    logger.info(f"âœ… APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™: {masked_key}")
    
    # aiohttp ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    try:
        import aiohttp
    except ImportError:
        logger.error("âŒ aiohttpãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        logger.error("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install aiohttp")
        return
        
    # ãƒ†ã‚¹ãƒˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«
    models_to_test = [
        ("gpt-4o-mini", "GPT-4o-mini (Quick thinking)"),
        ("gpt-4o", "GPT-4o (Deep thinking)"),
        ("gpt-3.5-turbo", "GPT-3.5 Turbo"),
        ("o3", "O3 (Advanced reasoning)"),
        ("o4-mini", "O4-mini (Fast reasoning)"),
    ]
    
    logger.info("\nğŸ“ ãƒ¢ãƒ‡ãƒ«åˆ©ç”¨å¯èƒ½æ€§ãƒ†ã‚¹ãƒˆ")
    logger.info("-" * 40)
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    async with aiohttp.ClientSession() as session:
        available_models = []
        
        for model_id, model_name in models_to_test:
            logger.info(f"\nãƒ†ã‚¹ãƒˆä¸­: {model_name} ({model_id})")
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£
            if model_id.startswith(('o3', 'o4')):
                # o-series models
                payload = {
                    "model": model_id,
                    "messages": [{"role": "user", "content": "Test. Reply with 'OK'."}],
                    "max_completion_tokens": 10,
                    "temperature": 1.0
                }
            else:
                # Standard models
                payload = {
                    "model": model_id,
                    "messages": [{"role": "user", "content": "Test. Reply with 'OK'."}],
                    "max_tokens": 10,
                    "temperature": 0
                }
            
            try:
                start_time = time.time()
                
                async with session.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    elapsed = time.time() - start_time
                    
                    if response.status == 200:
                        data = await response.json()
                        reply = data['choices'][0]['message']['content']
                        logger.info(f"   âœ… åˆ©ç”¨å¯èƒ½")
                        logger.info(f"   å¿œç­”æ™‚é–“: {elapsed:.2f}ç§’")
                        logger.info(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {reply}")
                        available_models.append(model_id)
                        
                    else:
                        error_text = await response.text()
                        error_data = json.loads(error_text) if error_text else {}
                        error_msg = error_data.get('error', {}).get('message', 'Unknown error')
                        
                        if "model_not_found" in error_msg or "does not exist" in error_msg:
                            logger.warning(f"   âš ï¸  ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        else:
                            logger.error(f"   âŒ ã‚¨ãƒ©ãƒ¼: {error_msg}")
                            
            except asyncio.TimeoutError:
                logger.error(f"   âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (30ç§’)")
            except Exception as e:
                logger.error(f"   âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
                
    # ã‚µãƒãƒªãƒ¼
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    logger.info("=" * 60)
    
    if available_models:
        logger.info(f"\nâœ… åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ« ({len(available_models)}å€‹):")
        for model in available_models:
            logger.info(f"   - {model}")
            
        logger.info("\nğŸ’¡ æ¨å¥¨è¨­å®š:")
        if "gpt-4o" in available_models:
            logger.info("   Deep Think Model: gpt-4o")
        elif "gpt-4" in available_models:
            logger.info("   Deep Think Model: gpt-4")
            
        if "gpt-4o-mini" in available_models:
            logger.info("   Quick Think Model: gpt-4o-mini")
        elif "gpt-3.5-turbo" in available_models:
            logger.info("   Quick Think Model: gpt-3.5-turbo")
            
    else:
        logger.error("\nâŒ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        logger.error("APIã‚­ãƒ¼ã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        
    # JSONå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
    if available_models:
        logger.info("\nğŸ“‹ JSONå½¢å¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ")
        logger.info("-" * 40)
        
        test_model = available_models[0]
        logger.info(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {test_model}")
        
        json_prompt = """Please respond with JSON format:
{
  "action": "HOLD",
  "confidence": 0.7,
  "reason": "Test response"
}"""
        
        payload = {
            "model": test_model,
            "messages": [{"role": "user", "content": json_prompt}],
            "max_tokens": 100,
            "temperature": 0
        }
        
        if test_model in ["gpt-4o-mini", "gpt-4o", "gpt-4-turbo"]:
            payload["response_format"] = {"type": "json_object"}
            
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        reply = data['choices'][0]['message']['content']
                        
                        # JSONãƒ‘ãƒ¼ã‚¹è©¦è¡Œ
                        try:
                            parsed = json.loads(reply)
                            logger.info("   âœ… JSONå½¢å¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹æˆåŠŸ")
                            logger.info(f"   ãƒ‘ãƒ¼ã‚¹çµæœ: {json.dumps(parsed, ensure_ascii=False)}")
                        except json.JSONDecodeError:
                            logger.warning("   âš ï¸  JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—")
                            logger.info(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {reply}")
                            
        except Exception as e:
            logger.error(f"   âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            
    logger.info("\nâœ… APIç–é€šãƒ†ã‚¹ãƒˆå®Œäº†")


async def test_with_backtest_llm():
    """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã§ä½¿ç”¨ã•ã‚Œã‚‹LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ”— Backtest2 LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆäº’æ›æ€§ãƒ†ã‚¹ãƒˆ")
    logger.info("=" * 60)
    
    try:
        # backtest2ã®LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        import sys
        sys.path.insert(0, '/Users/y-sato/TradingAgents2')
        
        from backtest2.agents.llm_client import OpenAILLMClient
        from backtest2.core.config import LLMConfig
        
        # é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ
        logger.info("\nğŸ“ é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã§ã®ãƒ†ã‚¹ãƒˆ")
        normal_config = LLMConfig(
            deep_think_model="gpt-4o",
            quick_think_model="gpt-4o-mini",
            temperature=0.7,
            max_tokens=100,
            timeout=30
        )
        
        client = OpenAILLMClient(normal_config)
        
        # ç°¡å˜ãªãƒ†ã‚¹ãƒˆ
        response = await client.generate(
            prompt="Test connection. Reply with 'Backtest LLM OK'.",
            context={},
            use_deep_thinking=False,
            agent_name="test_agent"
        )
        
        logger.info(f"   âœ… LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šæˆåŠŸ")
        logger.info(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response[:100]}")
        
        # ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã®ãƒ†ã‚¹ãƒˆ
        logger.info("\nğŸ“ ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§ã®ãƒ†ã‚¹ãƒˆ")
        mock_config = LLMConfig(
            deep_think_model="mock",
            quick_think_model="mock",
            temperature=0.0,
            timeout=30
        )
        
        mock_client = OpenAILLMClient(mock_config)
        mock_response = await mock_client.generate(
            prompt="Test mock mode",
            context={},
            agent_name="market_analyst"
        )
        
        logger.info(f"   âœ… ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰å‹•ä½œç¢ºèª")
        logger.info(f"   ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {mock_response[:200]}")
        
    except ImportError as e:
        logger.warning(f"   âš ï¸  Backtest2ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        logger.info("   ï¼ˆã“ã‚Œã¯ç’°å¢ƒä¾å­˜ã®å•é¡Œã§ã™ï¼‰")
    except Exception as e:
        logger.error(f"   âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")


if __name__ == "__main__":
    # åŸºæœ¬çš„ãªæ¥ç¶šãƒ†ã‚¹ãƒˆ
    asyncio.run(test_openai_connection())
    
    # Backtest LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆ
    asyncio.run(test_with_backtest_llm())