"""
E2Eãƒ†ã‚¹ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã¨æœ€é©åŒ–
å®Ÿè¡Œæ™‚é–“ã®æ¸¬å®šã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ç‰¹å®šã€æœ€é©åŒ–ææ¡ˆã‚’æä¾›
"""

import time
import psutil
import os
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Callable
from contextlib import contextmanager
import statistics
from dataclasses import dataclass, field
from functools import wraps
import threading
import gc


@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    test_name: str
    start_time: float
    end_time: float = 0
    duration: float = 0
    cpu_usage: List[float] = field(default_factory=list)
    memory_usage: List[float] = field(default_factory=list)
    network_io: Dict[str, int] = field(default_factory=dict)
    disk_io: Dict[str, int] = field(default_factory=dict)
    browser_metrics: Dict[str, Any] = field(default_factory=dict)
    custom_markers: Dict[str, float] = field(default_factory=dict)
    
    def calculate_duration(self):
        """å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—"""
        self.duration = self.end_time - self.start_time
    
    def get_summary(self) -> Dict[str, Any]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        return {
            "test_name": self.test_name,
            "duration": self.duration,
            "cpu_usage_avg": statistics.mean(self.cpu_usage) if self.cpu_usage else 0,
            "cpu_usage_max": max(self.cpu_usage) if self.cpu_usage else 0,
            "memory_usage_avg": statistics.mean(self.memory_usage) if self.memory_usage else 0,
            "memory_usage_max": max(self.memory_usage) if self.memory_usage else 0,
            "network_io": self.network_io,
            "disk_io": self.disk_io,
            "browser_metrics": self.browser_metrics,
            "custom_markers": self.custom_markers
        }


class PerformanceMonitor:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, report_dir: str = "reports/e2e/performance"):
        self.report_dir = Path(report_dir)
        self.report_dir.mkdir(parents=True, exist_ok=True)
        self.metrics: Dict[str, PerformanceMetrics] = {}
        self.monitoring_thread: Optional[threading.Thread] = None
        self.stop_monitoring = threading.Event()
        self.process = psutil.Process(os.getpid())
    
    @contextmanager
    def measure(self, test_name: str, page=None):
        """ãƒ†ã‚¹ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¸¬å®š"""
        metrics = PerformanceMetrics(
            test_name=test_name,
            start_time=time.time()
        )
        
        # åˆæœŸãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¨˜éŒ²
        self._record_initial_resources(metrics)
        
        # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        self.stop_monitoring.clear()
        self.monitoring_thread = threading.Thread(
            target=self._monitor_resources,
            args=(metrics,)
        )
        self.monitoring_thread.start()
        
        try:
            # ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åé›†ã‚’é–‹å§‹
            if page:
                self._setup_browser_monitoring(page, metrics)
            
            yield metrics
            
        finally:
            # ç›£è¦–ã‚’åœæ­¢
            self.stop_monitoring.set()
            if self.monitoring_thread:
                self.monitoring_thread.join(timeout=5)
            
            # æœ€çµ‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²
            metrics.end_time = time.time()
            metrics.calculate_duration()
            
            # ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†
            if page:
                self._collect_browser_metrics(page, metrics)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿å­˜
            self.metrics[test_name] = metrics
            self._save_metrics(metrics)
    
    def mark(self, metrics: PerformanceMetrics, marker_name: str):
        """ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ¼ã‚«ãƒ¼ã‚’è¿½åŠ """
        metrics.custom_markers[marker_name] = time.time() - metrics.start_time
    
    def _monitor_resources(self, metrics: PerformanceMetrics):
        """ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³ã‚’ç›£è¦–"""
        while not self.stop_monitoring.is_set():
            try:
                # CPUä½¿ç”¨ç‡
                cpu_percent = self.process.cpu_percent(interval=0.1)
                metrics.cpu_usage.append(cpu_percent)
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
                memory_info = self.process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024
                metrics.memory_usage.append(memory_mb)
                
                # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯I/O
                net_io = psutil.net_io_counters()
                metrics.network_io = {
                    "bytes_sent": net_io.bytes_sent,
                    "bytes_recv": net_io.bytes_recv
                }
                
                # ãƒ‡ã‚£ã‚¹ã‚¯I/O
                disk_io = psutil.disk_io_counters()
                if disk_io:
                    metrics.disk_io = {
                        "read_bytes": disk_io.read_bytes,
                        "write_bytes": disk_io.write_bytes
                    }
                
            except Exception as e:
                print(f"ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
            
            time.sleep(0.5)
    
    def _record_initial_resources(self, metrics: PerformanceMetrics):
        """åˆæœŸãƒªã‚½ãƒ¼ã‚¹çŠ¶æ…‹ã‚’è¨˜éŒ²"""
        try:
            # åˆæœŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯I/O
            net_io = psutil.net_io_counters()
            metrics.network_io["initial_bytes_sent"] = net_io.bytes_sent
            metrics.network_io["initial_bytes_recv"] = net_io.bytes_recv
            
            # åˆæœŸãƒ‡ã‚£ã‚¹ã‚¯I/O
            disk_io = psutil.disk_io_counters()
            if disk_io:
                metrics.disk_io["initial_read_bytes"] = disk_io.read_bytes
                metrics.disk_io["initial_write_bytes"] = disk_io.write_bytes
        except:
            pass
    
    def _setup_browser_monitoring(self, page, metrics: PerformanceMetrics):
        """ãƒ–ãƒ©ã‚¦ã‚¶ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’è¨­å®š"""
        try:
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¨ãƒ³ãƒˆãƒªã®åé›†ã‚’æœ‰åŠ¹åŒ–
            page.evaluate("""
                window.__performanceEntries = [];
                const observer = new PerformanceObserver((list) => {
                    window.__performanceEntries.push(...list.getEntries());
                });
                observer.observe({ entryTypes: ['navigation', 'resource', 'measure'] });
            """)
        except:
            pass
    
    def _collect_browser_metrics(self, page, metrics: PerformanceMetrics):
        """ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†"""
        try:
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒŸãƒ³ã‚°
            timing = page.evaluate("""
                () => {
                    const nav = performance.getEntriesByType('navigation')[0];
                    return nav ? {
                        domContentLoaded: nav.domContentLoadedEventEnd - nav.domContentLoadedEventStart,
                        loadComplete: nav.loadEventEnd - nav.loadEventStart,
                        domInteractive: nav.domInteractive - nav.fetchStart,
                        responseTime: nav.responseEnd - nav.requestStart
                    } : {};
                }
            """)
            metrics.browser_metrics["timing"] = timing
            
            # ãƒªã‚½ãƒ¼ã‚¹èª­ã¿è¾¼ã¿çµ±è¨ˆ
            resources = page.evaluate("""
                () => {
                    const resources = performance.getEntriesByType('resource');
                    const stats = {
                        total: resources.length,
                        byType: {},
                        slowest: []
                    };
                    
                    resources.forEach(r => {
                        const type = r.initiatorType || 'other';
                        stats.byType[type] = (stats.byType[type] || 0) + 1;
                    });
                    
                    stats.slowest = resources
                        .sort((a, b) => b.duration - a.duration)
                        .slice(0, 5)
                        .map(r => ({
                            name: r.name.split('/').pop(),
                            duration: Math.round(r.duration),
                            type: r.initiatorType
                        }));
                    
                    return stats;
                }
            """)
            metrics.browser_metrics["resources"] = resources
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆChromeé™å®šï¼‰
            memory = page.evaluate("""
                () => {
                    if (performance.memory) {
                        return {
                            usedJSHeapSize: Math.round(performance.memory.usedJSHeapSize / 1024 / 1024),
                            totalJSHeapSize: Math.round(performance.memory.totalJSHeapSize / 1024 / 1024),
                            jsHeapSizeLimit: Math.round(performance.memory.jsHeapSizeLimit / 1024 / 1024)
                        };
                    }
                    return null;
                }
            """)
            if memory:
                metrics.browser_metrics["memory"] = memory
            
        except Exception as e:
            print(f"ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_metrics(self, metrics: PerformanceMetrics):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{metrics.test_name}_{timestamp}.json"
        filepath = self.report_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(metrics.get_summary(), f, ensure_ascii=False, indent=2)
    
    def generate_performance_report(self) -> Path:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        all_metrics = []
        
        # ä¿å­˜ã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿
        for json_file in self.report_dir.glob("*.json"):
            with open(json_file, 'r') as f:
                all_metrics.append(json.load(f))
        
        # HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        html_content = self._generate_html_report(all_metrics)
        
        report_path = self.report_dir / "performance_report.html"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return report_path
    
    def _generate_html_report(self, metrics_list: List[Dict]) -> str:
        """HTMLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        # ãƒ†ã‚¹ãƒˆåˆ¥ã«é›†è¨ˆ
        test_summary = {}
        for metrics in metrics_list:
            test_name = metrics["test_name"]
            if test_name not in test_summary:
                test_summary[test_name] = {
                    "runs": [],
                    "durations": [],
                    "cpu_usage": [],
                    "memory_usage": []
                }
            
            test_summary[test_name]["runs"].append(metrics)
            test_summary[test_name]["durations"].append(metrics["duration"])
            test_summary[test_name]["cpu_usage"].append(metrics["cpu_usage_avg"])
            test_summary[test_name]["memory_usage"].append(metrics["memory_usage_avg"])
        
        html = f"""
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>E2E Test Performance Report</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 30px;
        }}
        .summary-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .metric-card {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        .metric-title {{
            font-size: 0.9em;
            color: #666;
            margin-bottom: 5px;
        }}
        .metric-value {{
            font-size: 2em;
            font-weight: bold;
            color: #333;
        }}
        .metric-unit {{
            font-size: 0.8em;
            color: #999;
        }}
        .chart-container {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin-bottom: 20px;
        }}
        .test-details {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin-bottom: 20px;
        }}
        .performance-table {{
            width: 100%;
            border-collapse: collapse;
        }}
        .performance-table th,
        .performance-table td {{
            padding: 10px;
            text-align: left;
            border-bottom: 1px solid #eee;
        }}
        .performance-table th {{
            background: #f8f9fa;
            font-weight: bold;
        }}
        .slow {{ color: #dc3545; }}
        .medium {{ color: #ffc107; }}
        .fast {{ color: #28a745; }}
        canvas {{
            max-height: 400px;
        }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸš€ E2E Test Performance Report</h1>
        <p>ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
        <p>ç·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ•°: {len(metrics_list)}</p>
    </div>
    
    <div class="summary-grid">
        <div class="metric-card">
            <div class="metric-title">å¹³å‡å®Ÿè¡Œæ™‚é–“</div>
            <div class="metric-value">
                {statistics.mean([m['duration'] for m in metrics_list]):.2f}
                <span class="metric-unit">ç§’</span>
            </div>
        </div>
        <div class="metric-card">
            <div class="metric-title">æœ€å¤§å®Ÿè¡Œæ™‚é–“</div>
            <div class="metric-value">
                {max([m['duration'] for m in metrics_list]):.2f}
                <span class="metric-unit">ç§’</span>
            </div>
        </div>
        <div class="metric-card">
            <div class="metric-title">å¹³å‡CPUä½¿ç”¨ç‡</div>
            <div class="metric-value">
                {statistics.mean([m['cpu_usage_avg'] for m in metrics_list]):.1f}
                <span class="metric-unit">%</span>
            </div>
        </div>
        <div class="metric-card">
            <div class="metric-title">å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡</div>
            <div class="metric-value">
                {statistics.mean([m['memory_usage_avg'] for m in metrics_list]):.0f}
                <span class="metric-unit">MB</span>
            </div>
        </div>
    </div>
    
    <div class="chart-container">
        <h2>å®Ÿè¡Œæ™‚é–“ã®æ¨ç§»</h2>
        <canvas id="durationChart"></canvas>
    </div>
    
    <div class="chart-container">
        <h2>ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³</h2>
        <canvas id="resourceChart"></canvas>
    </div>
    
    <div class="test-details">
        <h2>ãƒ†ã‚¹ãƒˆåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹</h2>
        <table class="performance-table">
            <thead>
                <tr>
                    <th>ãƒ†ã‚¹ãƒˆå</th>
                    <th>å®Ÿè¡Œå›æ•°</th>
                    <th>å¹³å‡æ™‚é–“</th>
                    <th>æœ€å°/æœ€å¤§</th>
                    <th>CPUä½¿ç”¨ç‡</th>
                    <th>ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡</th>
                </tr>
            </thead>
            <tbody>
"""
        
        for test_name, summary in test_summary.items():
            avg_duration = statistics.mean(summary["durations"])
            min_duration = min(summary["durations"])
            max_duration = max(summary["durations"])
            avg_cpu = statistics.mean(summary["cpu_usage"])
            avg_memory = statistics.mean(summary["memory_usage"])
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
            perf_class = "fast" if avg_duration < 5 else "medium" if avg_duration < 10 else "slow"
            
            html += f"""
                <tr>
                    <td>{test_name}</td>
                    <td>{len(summary["runs"])}</td>
                    <td class="{perf_class}">{avg_duration:.2f}ç§’</td>
                    <td>{min_duration:.2f} / {max_duration:.2f}</td>
                    <td>{avg_cpu:.1f}%</td>
                    <td>{avg_memory:.0f}MB</td>
                </tr>
"""
        
        html += """
            </tbody>
        </table>
    </div>
    
    <script>
        // å®Ÿè¡Œæ™‚é–“ãƒãƒ£ãƒ¼ãƒˆ
        const durationCtx = document.getElementById('durationChart').getContext('2d');
        new Chart(durationCtx, {
            type: 'line',
            data: {
                labels: """ + json.dumps([m["test_name"] for m in metrics_list[-20:]]) + """,
                datasets: [{
                    label: 'å®Ÿè¡Œæ™‚é–“ (ç§’)',
                    data: """ + json.dumps([m["duration"] for m in metrics_list[-20:]]) + """,
                    borderColor: 'rgb(75, 192, 192)',
                    tension: 0.1
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false
            }
        });
        
        // ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ãƒãƒ£ãƒ¼ãƒˆ
        const resourceCtx = document.getElementById('resourceChart').getContext('2d');
        new Chart(resourceCtx, {
            type: 'bar',
            data: {
                labels: """ + json.dumps(list(test_summary.keys())[:10]) + """,
                datasets: [{
                    label: 'CPUä½¿ç”¨ç‡ (%)',
                    data: """ + json.dumps([statistics.mean(s["cpu_usage"]) for s in list(test_summary.values())[:10]]) + """,
                    backgroundColor: 'rgba(255, 99, 132, 0.5)',
                    yAxisID: 'y-cpu'
                }, {
                    label: 'ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)',
                    data: """ + json.dumps([statistics.mean(s["memory_usage"]) for s in list(test_summary.values())[:10]]) + """,
                    backgroundColor: 'rgba(54, 162, 235, 0.5)',
                    yAxisID: 'y-memory'
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                scales: {
                    'y-cpu': {
                        type: 'linear',
                        display: true,
                        position: 'left',
                        title: {
                            display: true,
                            text: 'CPU (%)'
                        }
                    },
                    'y-memory': {
                        type: 'linear',
                        display: true,
                        position: 'right',
                        title: {
                            display: true,
                            text: 'Memory (MB)'
                        },
                        grid: {
                            drawOnChartArea: false
                        }
                    }
                }
            }
        });
    </script>
</body>
</html>
"""
        
        return html
    
    def analyze_bottlenecks(self) -> Dict[str, Any]:
        """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’åˆ†æ"""
        bottlenecks = {
            "slow_tests": [],
            "high_cpu_tests": [],
            "high_memory_tests": [],
            "recommendations": []
        }
        
        for test_name, metrics in self.metrics.items():
            summary = metrics.get_summary()
            
            # é…ã„ãƒ†ã‚¹ãƒˆï¼ˆ10ç§’ä»¥ä¸Šï¼‰
            if summary["duration"] > 10:
                bottlenecks["slow_tests"].append({
                    "test": test_name,
                    "duration": summary["duration"],
                    "markers": summary["custom_markers"]
                })
            
            # é«˜CPUä½¿ç”¨ç‡ï¼ˆ80%ä»¥ä¸Šï¼‰
            if summary["cpu_usage_max"] > 80:
                bottlenecks["high_cpu_tests"].append({
                    "test": test_name,
                    "cpu_max": summary["cpu_usage_max"],
                    "cpu_avg": summary["cpu_usage_avg"]
                })
            
            # é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨ï¼ˆ500MBä»¥ä¸Šï¼‰
            if summary["memory_usage_max"] > 500:
                bottlenecks["high_memory_tests"].append({
                    "test": test_name,
                    "memory_max": summary["memory_usage_max"],
                    "memory_avg": summary["memory_usage_avg"]
                })
        
        # æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ
        if bottlenecks["slow_tests"]:
            bottlenecks["recommendations"].append(
                "é…ã„ãƒ†ã‚¹ãƒˆã‚’ä¸¦åˆ—å®Ÿè¡Œã¾ãŸã¯åˆ†å‰²ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
            )
        
        if bottlenecks["high_cpu_tests"]:
            bottlenecks["recommendations"].append(
                "CPUè² è·ã®é«˜ã„ãƒ†ã‚¹ãƒˆã§ã¯ã€å¾…æ©Ÿå‡¦ç†ã‚„éåŒæœŸå‡¦ç†ã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„"
            )
        
        if bottlenecks["high_memory_tests"]:
            bottlenecks["recommendations"].append(
                "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å¤šã„ãƒ†ã‚¹ãƒˆã§ã¯ã€ä¸è¦ãªãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„"
            )
        
        return bottlenecks


# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼
def measure_performance(monitor: PerformanceMonitor):
    """é–¢æ•°ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¸¬å®šã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            test_name = f"{func.__module__}.{func.__name__}"
            with monitor.measure(test_name) as metrics:
                result = func(*args, **kwargs)
                return result
        return wrapper
    return decorator


# ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
class MemoryOptimizer:
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€é©åŒ–ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
    
    @staticmethod
    def cleanup():
        """ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¼·åˆ¶å®Ÿè¡Œ"""
        gc.collect()
    
    @staticmethod
    def get_memory_usage() -> float:
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—ï¼ˆMBï¼‰"""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024
    
    @staticmethod
    @contextmanager
    def memory_limit(max_mb: float):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åˆ¶é™"""
        initial_memory = MemoryOptimizer.get_memory_usage()
        
        yield
        
        current_memory = MemoryOptimizer.get_memory_usage()
        memory_increase = current_memory - initial_memory
        
        if memory_increase > max_mb:
            MemoryOptimizer.cleanup()
            raise MemoryError(
                f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶é™ã‚’è¶…ãˆã¾ã—ãŸ: "
                f"{memory_increase:.1f}MB > {max_mb}MB"
            )