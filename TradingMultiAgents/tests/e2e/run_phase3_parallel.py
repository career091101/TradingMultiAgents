#!/usr/bin/env python3
"""
Phase 3 ä¸¦åˆ—ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã‚«ãƒ†ã‚´ãƒªåˆ¥ã€ãƒ–ãƒ©ã‚¦ã‚¶åˆ¥ã®ä¸¦åˆ—å®Ÿè¡Œã‚’æœ€é©åŒ–
"""

import argparse
import asyncio
import json
import os
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import List, Dict, Any, Optional
import subprocess
import multiprocessing


def parse_arguments():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æ"""
    parser = argparse.ArgumentParser(
        description="Phase 3 ä¸¦åˆ—E2Eãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"
    )
    parser.add_argument(
        "--categories", 
        nargs='+',
        default=['error_handling', 'performance', 'security'],
        help="å®Ÿè¡Œã™ã‚‹ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒª"
    )
    parser.add_argument(
        "--browsers", 
        nargs='+',
        default=['chromium'],
        help="ä½¿ç”¨ã™ã‚‹ãƒ–ãƒ©ã‚¦ã‚¶"
    )
    parser.add_argument(
        "--parallel-workers", 
        type=int,
        default=min(4, multiprocessing.cpu_count()),
        help="ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°"
    )
    parser.add_argument(
        "--output-dir", 
        default="tests/reports/phase3_parallel",
        help="çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--timeout", 
        type=int,
        default=600,
        help="å„ãƒ†ã‚¹ãƒˆã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰"
    )
    parser.add_argument(
        "--retry-failed", 
        type=int,
        default=1,
        help="å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®å†è©¦è¡Œå›æ•°"
    )
    parser.add_argument(
        "--fast-fail", 
        action='store_true',
        help="æœ€åˆã®å¤±æ•—ã§å…¨ä½“ã‚’åœæ­¢"
    )
    return parser.parse_args()


class TestRunner:
    """ä¸¦åˆ—ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, args):
        self.args = args
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.test_configs = self._generate_test_configs()
        self.results = {}
        self.start_time = time.time()
    
    def _generate_test_configs(self) -> List[Dict[str, Any]]:
        """ãƒ†ã‚¹ãƒˆè¨­å®šã®çµ„ã¿åˆã‚ã›ã‚’ç”Ÿæˆ"""
        configs = []
        
        for category in self.args.categories:
            for browser in self.args.browsers:
                config = {
                    'category': category,
                    'browser': browser,
                    'test_file': f"test_{category}_adapted.py",
                    'output_file': f"{category}_{browser}_results.json",
                    'log_file': f"{category}_{browser}.log",
                    'screenshot_dir': f"screenshots_{category}_{browser}"
                }
                configs.append(config)
        
        return configs
    
    def _run_single_test(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """å˜ä¸€ãƒ†ã‚¹ãƒˆè¨­å®šã‚’å®Ÿè¡Œ"""
        test_id = f"{config['category']}_{config['browser']}"
        print(f"ğŸš€ ãƒ†ã‚¹ãƒˆé–‹å§‹: {test_id}")
        
        start_time = time.time()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æº–å‚™
        test_output_dir = self.output_dir / test_id
        test_output_dir.mkdir(parents=True, exist_ok=True)
        
        # pytestã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰
        cmd = [
            "python", "-m", "pytest",
            f"tests/e2e/{config['test_file']}",
            f"--browser={config['browser']}",
            "--screenshot=on",
            f"--html={test_output_dir / 'report.html'}",
            "--self-contained-html",
            f"--json-report={test_output_dir / config['output_file']}",
            "--json-report-summary",
            "-v"
        ]
        
        # CIç’°å¢ƒã®è¨­å®š
        env = os.environ.copy()
        env.update({
            'CI': 'true',
            'PYTEST_CURRENT_TEST': test_id,
            'SCREENSHOT_DIR': str(test_output_dir / config['screenshot_dir'])
        })
        
        result = {
            'config': config,
            'test_id': test_id,
            'start_time': start_time,
            'status': 'unknown',
            'return_code': -1,
            'stdout': '',
            'stderr': '',
            'duration': 0,
            'output_dir': str(test_output_dir)
        }
        
        try:
            # ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
            process = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=self.args.timeout,
                env=env,
                cwd=Path.cwd()
            )
            
            result.update({
                'return_code': process.returncode,
                'stdout': process.stdout,
                'stderr': process.stderr,
                'status': 'passed' if process.returncode == 0 else 'failed'
            })
            
            # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’æŠ½å‡º
            self._extract_test_details(result, test_output_dir)
            
        except subprocess.TimeoutExpired:
            result.update({
                'status': 'timeout',
                'stderr': f"Test timed out after {self.args.timeout} seconds"
            })
            print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {test_id}")
            
        except Exception as e:
            result.update({
                'status': 'error',
                'stderr': str(e)
            })
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {test_id} - {e}")
        
        finally:
            result['duration'] = time.time() - start_time
            
        # çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
        status_emoji = {
            'passed': 'âœ…',
            'failed': 'âŒ', 
            'timeout': 'â°',
            'error': 'ğŸ’¥'
        }.get(result['status'], 'â“')
        
        print(f"{status_emoji} ãƒ†ã‚¹ãƒˆå®Œäº†: {test_id} ({result['duration']:.1f}s)")
        
        return result
    
    def _extract_test_details(self, result: Dict, output_dir: Path):
        """ãƒ†ã‚¹ãƒˆçµæœã‹ã‚‰è©³ç´°æƒ…å ±ã‚’æŠ½å‡º"""
        json_file = output_dir / result['config']['output_file']
        
        if json_file.exists():
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                summary = data.get('summary', {})
                result['test_details'] = {
                    'total': summary.get('total', 0),
                    'passed': summary.get('passed', 0),
                    'failed': summary.get('failed', 0),
                    'skipped': summary.get('skipped', 0),
                    'error': summary.get('error', 0)
                }
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                if 'performance' in result['config']['category']:
                    result['performance'] = self._extract_performance_data(data)
                
                # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                if 'security' in result['config']['category']:
                    result['security'] = self._extract_security_data(data)
                    
            except Exception as e:
                print(f"Warning: {json_file}ã®è©³ç´°æŠ½å‡ºã«å¤±æ•—: {e}")
    
    def _extract_performance_data(self, data: Dict) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        performance = {}
        
        tests = data.get('tests', [])
        page_loads = []
        navigations = []
        
        for test in tests:
            output = test.get('stdout', '') + test.get('stderr', '')
            
            # ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰æ™‚é–“
            import re
            page_load_match = re.search(r'Page load time: ([\d.]+)', output)
            if page_load_match:
                page_loads.append(float(page_load_match.group(1)))
            
            # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³æ™‚é–“
            nav_match = re.search(r'Navigation time: ([\d.]+)', output)
            if nav_match:
                navigations.append(float(nav_match.group(1)))
        
        if page_loads:
            performance['avg_page_load'] = sum(page_loads) / len(page_loads)
            performance['max_page_load'] = max(page_loads)
        
        if navigations:
            performance['avg_navigation'] = sum(navigations) / len(navigations)
        
        return performance
    
    def _extract_security_data(self, data: Dict) -> Dict[str, Any]:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        security = {
            'vulnerabilities': 0,
            'warnings': 0,
            'security_tests': 0
        }
        
        tests = data.get('tests', [])
        for test in tests:
            if 'security' in test.get('name', '').lower():
                security['security_tests'] += 1
                
                output = test.get('stdout', '') + test.get('stderr', '')
                if 'vulnerability' in output.lower():
                    security['vulnerabilities'] += 1
                if 'warning' in output.lower():
                    security['warnings'] += 1
        
        return security
    
    def _retry_failed_tests(self, failed_results: List[Dict]) -> List[Dict]:
        """å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã‚’å†è©¦è¡Œ"""
        if not failed_results or self.args.retry_failed <= 0:
            return failed_results
        
        print(f"ğŸ”„ {len(failed_results)}å€‹ã®å¤±æ•—ãƒ†ã‚¹ãƒˆã‚’å†è©¦è¡Œä¸­...")
        
        retry_results = []
        with ThreadPoolExecutor(max_workers=min(2, self.args.parallel_workers)) as executor:
            future_to_config = {
                executor.submit(self._run_single_test, result['config']): result 
                for result in failed_results
            }
            
            for future in as_completed(future_to_config):
                retry_result = future.result()
                retry_results.append(retry_result)
        
        return retry_results
    
    def run_all_tests(self) -> Dict[str, Any]:
        """å…¨ãƒ†ã‚¹ãƒˆã‚’ä¸¦åˆ—å®Ÿè¡Œ"""
        print(f"ğŸš€ Phase 3 ä¸¦åˆ—ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print(f"   ğŸ“Š è¨­å®šæ•°: {len(self.test_configs)}")
        print(f"   ğŸ‘¥ ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼: {self.args.parallel_workers}")
        print(f"   â±ï¸  ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {self.args.timeout}ç§’")
        
        all_results = []
        failed_results = []
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        with ThreadPoolExecutor(max_workers=self.args.parallel_workers) as executor:
            future_to_config = {
                executor.submit(self._run_single_test, config): config 
                for config in self.test_configs
            }
            
            for future in as_completed(future_to_config):
                result = future.result()
                all_results.append(result)
                
                if result['status'] not in ['passed']:
                    failed_results.append(result)
                    
                    if self.args.fast_fail:
                        print(f"ğŸ’¥ Fast-failæœ‰åŠ¹: {result['test_id']}ã§åœæ­¢")
                        # æ®‹ã‚Šã®futureã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
                        for remaining_future in future_to_config:
                            if not remaining_future.done():
                                remaining_future.cancel()
                        break
        
        # å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®å†è©¦è¡Œ
        if failed_results and not self.args.fast_fail:
            retry_results = self._retry_failed_tests(failed_results)
            
            # å…ƒã®çµæœã‚’å†è©¦è¡Œçµæœã§ç½®ãæ›ãˆ
            for retry_result in retry_results:
                for i, original_result in enumerate(all_results):
                    if original_result['test_id'] == retry_result['test_id']:
                        all_results[i] = retry_result
                        break
        
        # ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
        summary = self._generate_summary(all_results)
        
        # çµæœã‚’ä¿å­˜
        self._save_results(all_results, summary)
        
        return {
            'results': all_results,
            'summary': summary
        }
    
    def _generate_summary(self, results: List[Dict]) -> Dict[str, Any]:
        """å®Ÿè¡Œã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        total_tests = len(results)
        passed_tests = len([r for r in results if r['status'] == 'passed'])
        failed_tests = len([r for r in results if r['status'] == 'failed'])
        timeout_tests = len([r for r in results if r['status'] == 'timeout'])
        error_tests = len([r for r in results if r['status'] == 'error'])
        
        total_duration = time.time() - self.start_time
        avg_duration = sum(r['duration'] for r in results) / total_tests if total_tests > 0 else 0
        
        summary = {
            'total_tests': total_tests,
            'passed': passed_tests,
            'failed': failed_tests,
            'timeout': timeout_tests,
            'error': error_tests,
            'success_rate': (passed_tests / total_tests * 100) if total_tests > 0 else 0,
            'total_duration': total_duration,
            'avg_test_duration': avg_duration,
            'parallel_workers': self.args.parallel_workers,
            'categories': list(set(r['config']['category'] for r in results)),
            'browsers': list(set(r['config']['browser'] for r in results))
        }
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        category_stats = {}
        for category in summary['categories']:
            cat_results = [r for r in results if r['config']['category'] == category]
            category_stats[category] = {
                'total': len(cat_results),
                'passed': len([r for r in cat_results if r['status'] == 'passed']),
                'failed': len([r for r in cat_results if r['status'] != 'passed'])
            }
        summary['category_stats'] = category_stats
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        perf_results = [r for r in results if r['config']['category'] == 'performance' and 'performance' in r]
        if perf_results:
            all_page_loads = [p['avg_page_load'] for r in perf_results for p in [r['performance']] if 'avg_page_load' in p]
            if all_page_loads:
                summary['performance'] = {
                    'avg_page_load': sum(all_page_loads) / len(all_page_loads),
                    'max_page_load': max(all_page_loads)
                }
        
        return summary
    
    def _save_results(self, results: List[Dict], summary: Dict):
        """çµæœã‚’ä¿å­˜"""
        # è©³ç´°çµæœ
        results_file = self.output_dir / "detailed_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'summary': summary,
                'results': results,
                'timestamp': time.time(),
                'args': vars(self.args)
            }, f, indent=2, ensure_ascii=False)
        
        # ã‚µãƒãƒªãƒ¼
        summary_file = self.output_dir / "summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ çµæœã‚’ä¿å­˜: {results_file}")
        print(f"ğŸ“Š ã‚µãƒãƒªãƒ¼: {summary_file}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    args = parse_arguments()
    
    # ç’°å¢ƒãƒã‚§ãƒƒã‚¯
    if not Path("tests/e2e").exists():
        print("âŒ tests/e2eãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)
    
    # ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼ã‚’ä½œæˆã—ã¦å®Ÿè¡Œ
    runner = TestRunner(args)
    result = runner.run_all_tests()
    
    summary = result['summary']
    
    # çµæœã‚’å‡ºåŠ›
    print("\n" + "="*60)
    print("ğŸ“Š Phase 3 ä¸¦åˆ—ãƒ†ã‚¹ãƒˆå®Ÿè¡Œçµæœ")
    print("="*60)
    print(f"â±ï¸  ç·å®Ÿè¡Œæ™‚é–“: {summary['total_duration']:.1f}ç§’")
    print(f"ğŸ“Š ç·ãƒ†ã‚¹ãƒˆæ•°: {summary['total_tests']}")
    print(f"âœ… æˆåŠŸ: {summary['passed']}")
    print(f"âŒ å¤±æ•—: {summary['failed']}")
    print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {summary['timeout']}")
    print(f"ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {summary['error']}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {summary['success_rate']:.1f}%")
    print(f"ğŸ‘¥ ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼: {summary['parallel_workers']}")
    print(f"âš¡ å¹³å‡ãƒ†ã‚¹ãƒˆæ™‚é–“: {summary['avg_test_duration']:.1f}ç§’")
    
    if 'performance' in summary:
        print(f"ğŸš€ å¹³å‡ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰: {summary['performance']['avg_page_load']:.2f}ç§’")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµæœ
    print("\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥çµæœ:")
    for category, stats in summary['category_stats'].items():
        success_rate = (stats['passed'] / stats['total'] * 100) if stats['total'] > 0 else 0
        print(f"   {category}: {stats['passed']}/{stats['total']} ({success_rate:.0f}%)")
    
    # çµ‚äº†ã‚³ãƒ¼ãƒ‰æ±ºå®š
    if summary['failed'] > 0 or summary['error'] > 0:
        print("\nâŒ ãƒ†ã‚¹ãƒˆã«å¤±æ•—ãŒã‚ã‚Šã¾ã™")
        sys.exit(1)
    else:
        print("\nâœ… å…¨ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()