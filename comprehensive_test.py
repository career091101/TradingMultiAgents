#!/usr/bin/env python
"""åŒ…æ‹¬çš„ãªJSONãƒ‘ãƒ¼ã‚·ãƒ³ã‚°ä¿®æ­£ãƒ†ã‚¹ãƒˆ"""

import os
import asyncio
import logging
from datetime import datetime
from pathlib import Path
import json
from collections import Counter

# APIã‚­ãƒ¼è¨­å®š
bashrc_path = Path.home() / ".bashrc"
if bashrc_path.exists():
    with open(bashrc_path, 'r') as f:
        for line in f:
            if 'export OPENAI_API_KEY=' in line:
                key = line.strip().split('=', 1)[1].strip('"')
                os.environ['OPENAI_API_KEY'] = key
                break

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('comprehensive_test.log'),
        logging.StreamHandler()
    ]
)

from backtest2.core.config import BacktestConfig, AgentConfig, LLMConfig, RiskConfig, TimeRange, RiskProfile
from backtest2.core.engine import BacktestEngine
from backtest2.agents.llm_client import OpenAILLMClient

async def test_json_parsing():
    """JSONãƒ‘ãƒ¼ã‚·ãƒ³ã‚°æ©Ÿèƒ½ã®å€‹åˆ¥ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ§ª JSONãƒ‘ãƒ¼ã‚·ãƒ³ã‚°æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    config = LLMConfig(
        deep_think_model="gpt-3.5-turbo",
        quick_think_model="gpt-3.5-turbo",
        temperature=0.7,
        max_tokens=300
    )
    
    client = OpenAILLMClient(config)
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_cases = [
        {
            "name": "Risk Manager Test",
            "agent": "Risk Manager",
            "prompt": "Strong buy signal detected",
            "schema": {
                "action": {"type": "string", "enum": ["BUY", "SELL", "HOLD"]},
                "confidence": {"type": "number"},
                "rationale": {"type": "string"}
            }
        },
        {
            "name": "Complex Schema Test",
            "agent": "Research Manager",
            "prompt": "Analyze bull vs bear thesis",
            "schema": {
                "investment_decision": {"type": "string", "enum": ["BUY", "SELL", "HOLD"]},
                "investment_plan": {"type": "object", "properties": {
                    "action": {"type": "string"},
                    "target_allocation": {"type": "number"}
                }},
                "confidence_level": {"type": "number"}
            }
        }
    ]
    
    results = []
    for test in test_cases:
        print(f"\nğŸ“ {test['name']}")
        try:
            result = await client.generate_structured(
                prompt=test['prompt'],
                context={"test": True},
                output_schema=test['schema'],
                agent_name=test['agent'],
                use_cache=False
            )
            print(f"âœ… æˆåŠŸ: {result}")
            results.append(("success", result))
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            results.append(("error", str(e)))
    
    await client.cleanup()
    return results

async def test_backtest_decisions():
    """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã§ã®æ±ºå®šç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ¯ ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæ±ºå®šç”Ÿæˆãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    config = BacktestConfig(
        symbols=["AAPL"],
        time_range=TimeRange(
            start=datetime(2025, 7, 23),
            end=datetime(2025, 7, 24)  # 1æ—¥ã®ã¿
        ),
        initial_capital=100000.0,
        debug=False,
        agent_config=AgentConfig(
            llm_config=LLMConfig(
                deep_think_model="gpt-3.5-turbo",
                quick_think_model="gpt-3.5-turbo",
                temperature=0.8,
                max_tokens=400
            ),
            use_japanese_prompts=True,
            max_debate_rounds=1,
            max_risk_discuss_rounds=1,
            enable_memory=False
        ),
        risk_config=RiskConfig(
            max_positions=5,
            position_limits={
                'AAPL': 0.5,
                RiskProfile.AGGRESSIVE: 0.8,
                RiskProfile.NEUTRAL: 0.5,
                RiskProfile.CONSERVATIVE: 0.3
            }
        )
    )
    
    engine = BacktestEngine(config)
    
    # æ±ºå®šã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£
    decisions = []
    errors = []
    
    original_info = logging.getLogger('backtest2.agents.orchestrator').info
    original_error = logging.getLogger('backtest2.agents.llm_client').error
    
    def capture_info(msg):
        if "Final decision for" in msg:
            decisions.append(msg)
        original_info(msg)
    
    def capture_error(msg):
        if "Failed to parse" in msg:
            errors.append(msg)
        original_error(msg)
    
    logging.getLogger('backtest2.agents.orchestrator').info = capture_info
    logging.getLogger('backtest2.agents.llm_client').error = capture_error
    
    try:
        result = await engine.run()
        perf = result.agent_performance
        
        return {
            "total_decisions": perf.get('total_decisions', 0),
            "breakdown": perf.get('decision_breakdown', {}),
            "trades": perf.get('total_trades', 0),
            "decisions_captured": decisions,
            "parse_errors": len(errors)
        }
    except Exception as e:
        return {"error": str(e)}
    finally:
        await engine._cleanup()

async def analyze_debug_logs():
    """ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã®åˆ†æ"""
    print("\nğŸ“Š ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°åˆ†æ")
    print("=" * 50)
    
    debug_dir = Path("logs/llm_debug")
    if not debug_dir.exists():
        return {"error": "No debug logs found"}
    
    stats = {
        "total_files": 0,
        "successful_json": 0,
        "failed_json": 0,
        "decisions": Counter(),
        "agents": Counter()
    }
    
    for file in sorted(debug_dir.glob("*.json"))[-20:]:  # æœ€æ–°20ãƒ•ã‚¡ã‚¤ãƒ«
        stats["total_files"] += 1
        
        try:
            with open(file, 'r') as f:
                data = json.load(f)
                
            agent = data.get('agent', 'unknown')
            stats["agents"][agent] += 1
            
            response = data.get('response', '')
            
            # JSONè§£ææˆåŠŸã‚’ãƒã‚§ãƒƒã‚¯
            try:
                if response.strip().startswith('{'):
                    parsed = json.loads(response)
                    stats["successful_json"] += 1
                    
                    # æ±ºå®šã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                    for field in ['action', 'recommendation', 'investment_decision']:
                        if field in parsed and parsed[field] in ['BUY', 'SELL', 'HOLD']:
                            stats["decisions"][parsed[field]] += 1
                            break
            except:
                stats["failed_json"] += 1
                
        except Exception as e:
            print(f"Error reading {file.name}: {e}")
    
    return stats

async def main():
    """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
    print("ğŸ¯ åŒ…æ‹¬çš„ãªJSONãƒ‘ãƒ¼ã‚·ãƒ³ã‚°ä¿®æ­£ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    print()
    
    results = {
        "timestamp": datetime.now().isoformat(),
        "tests": {}
    }
    
    # 1. JSONãƒ‘ãƒ¼ã‚·ãƒ³ã‚°æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
    print("\n[1/3] JSONãƒ‘ãƒ¼ã‚·ãƒ³ã‚°æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ...")
    json_results = await test_json_parsing()
    success_count = sum(1 for r in json_results if r[0] == "success")
    results["tests"]["json_parsing"] = {
        "total": len(json_results),
        "success": success_count,
        "rate": f"{(success_count/len(json_results)*100):.1f}%"
    }
    
    # 2. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæ±ºå®šç”Ÿæˆãƒ†ã‚¹ãƒˆ
    print("\n[2/3] ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæ±ºå®šç”Ÿæˆãƒ†ã‚¹ãƒˆ...")
    backtest_results = await test_backtest_decisions()
    results["tests"]["backtest"] = backtest_results
    
    # 3. ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°åˆ†æ
    print("\n[3/3] ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°åˆ†æ...")
    debug_stats = await analyze_debug_logs()
    results["tests"]["debug_analysis"] = debug_stats
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "="*70)
    print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("="*70)
    
    # JSONãƒ‘ãƒ¼ã‚·ãƒ³ã‚°çµæœ
    print(f"\nâœ… JSONãƒ‘ãƒ¼ã‚·ãƒ³ã‚°æˆåŠŸç‡: {results['tests']['json_parsing']['rate']}")
    
    # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœ
    if 'error' not in backtest_results:
        total = backtest_results['total_decisions']
        breakdown = backtest_results['breakdown']
        print(f"\nğŸ“ˆ ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæ±ºå®šåˆ†å¸ƒ:")
        for action in ['BUY', 'SELL', 'HOLD']:
            count = breakdown.get(action, 0)
            if total > 0:
                pct = count / total * 100
                print(f"  {action}: {count} ({pct:.1f}%)")
            else:
                print(f"  {action}: {count}")
        
        print(f"\nğŸ’° å®Ÿè¡Œã•ã‚ŒãŸå–å¼•: {backtest_results['trades']}")
        print(f"ğŸ”§ ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {backtest_results['parse_errors']}")
    
    # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°çµ±è¨ˆ
    if 'error' not in debug_stats:
        print(f"\nğŸ“„ ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°çµ±è¨ˆ:")
        print(f"  ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {debug_stats['total_files']}")
        print(f"  JSONæˆåŠŸ: {debug_stats['successful_json']}")
        print(f"  JSONå¤±æ•—: {debug_stats['failed_json']}")
        
        if debug_stats['total_files'] > 0:
            success_rate = debug_stats['successful_json'] / debug_stats['total_files'] * 100
            print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
        
        print(f"\n  æ±ºå®šåˆ†å¸ƒ:")
        for action, count in debug_stats['decisions'].items():
            print(f"    {action}: {count}")
    
    # æœ€çµ‚è©•ä¾¡
    print("\n" + "="*70)
    print("ğŸ æœ€çµ‚è©•ä¾¡")
    print("="*70)
    
    if 'error' not in backtest_results:
        all_hold = breakdown.get('HOLD', 0) == total and total > 0
        has_variety = len([a for a in ['BUY', 'SELL', 'HOLD'] if breakdown.get(a, 0) > 0]) > 1
        
        if all_hold:
            print("âŒ å•é¡Œ: ã™ã¹ã¦ã®æ±ºå®šãŒHOLDã§ã™")
        elif has_variety:
            print("âœ… æˆåŠŸ: ã‚·ã‚¹ãƒ†ãƒ ã¯å¤šæ§˜ãªæ±ºå®šï¼ˆBUY/SELL/HOLDï¼‰ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™ï¼")
        else:
            print("âš ï¸  è­¦å‘Š: æ±ºå®šã®å¤šæ§˜æ€§ãŒé™å®šçš„ã§ã™")
    
    # çµæœã‚’ä¿å­˜
    with open('comprehensive_test_results.json', 'w') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ è©³ç´°ãªçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: comprehensive_test_results.json")

if __name__ == "__main__":
    asyncio.run(main())