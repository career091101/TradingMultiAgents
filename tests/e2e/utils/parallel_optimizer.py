"""
E2Eãƒ†ã‚¹ãƒˆã®ä¸¦åˆ—å®Ÿè¡Œã‚’æœ€é©åŒ–
ãƒ†ã‚¹ãƒˆã®ä¾å­˜é–¢ä¿‚ã‚’åˆ†æã—ã€æœ€é©ãªå®Ÿè¡Œé †åºã‚’æ±ºå®š
"""

import networkx as nx
from typing import List, Dict, Set, Tuple, Optional, Any
from dataclasses import dataclass, field
import json
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from collections import defaultdict


@dataclass
class TestNode:
    """ãƒ†ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    name: str
    module: str
    estimated_duration: float = 5.0
    resource_requirements: Dict[str, float] = field(default_factory=dict)
    dependencies: Set[str] = field(default_factory=set)
    tags: Set[str] = field(default_factory=set)
    priority: int = 0
    
    @property
    def id(self) -> str:
        return f"{self.module}::{self.name}"


class TestDependencyGraph:
    """ãƒ†ã‚¹ãƒˆã®ä¾å­˜é–¢ä¿‚ã‚°ãƒ©ãƒ•"""
    
    def __init__(self):
        self.graph = nx.DiGraph()
        self.test_nodes: Dict[str, TestNode] = {}
    
    def add_test(self, test: TestNode):
        """ãƒ†ã‚¹ãƒˆã‚’è¿½åŠ """
        self.test_nodes[test.id] = test
        self.graph.add_node(test.id, test=test)
    
    def add_dependency(self, from_test: str, to_test: str):
        """ä¾å­˜é–¢ä¿‚ã‚’è¿½åŠ """
        self.graph.add_edge(from_test, to_test)
    
    def get_execution_order(self) -> List[List[str]]:
        """ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½ãªã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‘ãŸå®Ÿè¡Œé †åºã‚’å–å¾—"""
        if not nx.is_directed_acyclic_graph(self.graph):
            raise ValueError("ãƒ†ã‚¹ãƒˆã®ä¾å­˜é–¢ä¿‚ã«å¾ªç’°ãŒã‚ã‚Šã¾ã™")
        
        # ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆã§ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—
        levels = defaultdict(list)
        for node in nx.topological_sort(self.graph):
            # ãƒãƒ¼ãƒ‰ã®ãƒ¬ãƒ™ãƒ«ï¼ˆä¾å­˜é–¢ä¿‚ã®æ·±ã•ï¼‰ã‚’è¨ˆç®—
            if self.graph.in_degree(node) == 0:
                level = 0
            else:
                level = max(levels[pred] for pred in self.graph.predecessors(node)) + 1
            
            levels[level].append(node)
        
        # ãƒ¬ãƒ™ãƒ«ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        execution_groups = []
        for level in sorted(levels.keys()):
            execution_groups.append(levels[level])
        
        return execution_groups
    
    def optimize_for_resources(self, max_workers: int) -> List[List[str]]:
        """ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã‚’è€ƒæ…®ã—ã¦æœ€é©åŒ–"""
        execution_groups = self.get_execution_order()
        optimized_groups = []
        
        for group in execution_groups:
            # ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®ãƒ†ã‚¹ãƒˆã‚’ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã§ã‚½ãƒ¼ãƒˆ
            tests = [(test_id, self.test_nodes[test_id]) for test_id in group]
            tests.sort(key=lambda x: (
                -x[1].priority,  # å„ªå…ˆåº¦ãŒé«˜ã„é †
                -x[1].estimated_duration  # å®Ÿè¡Œæ™‚é–“ãŒé•·ã„é †
            ))
            
            # ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ã‚’è€ƒæ…®ã—ã¦ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰²
            subgroups = self._partition_by_resources(tests, max_workers)
            optimized_groups.extend(subgroups)
        
        return optimized_groups
    
    def _partition_by_resources(self, 
                               tests: List[Tuple[str, TestNode]], 
                               max_workers: int) -> List[List[str]]:
        """ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ã«åŸºã¥ã„ã¦ãƒ†ã‚¹ãƒˆã‚’åˆ†å‰²"""
        subgroups = []
        current_group = []
        current_resources = defaultdict(float)
        
        for test_id, test_node in tests:
            # ãƒªã‚½ãƒ¼ã‚¹è¦ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
            can_add = True
            for resource, requirement in test_node.resource_requirements.items():
                if current_resources[resource] + requirement > 1.0:
                    can_add = False
                    break
            
            if can_add and len(current_group) < max_workers:
                current_group.append(test_id)
                for resource, requirement in test_node.resource_requirements.items():
                    current_resources[resource] += requirement
            else:
                if current_group:
                    subgroups.append(current_group)
                current_group = [test_id]
                current_resources = defaultdict(float)
                for resource, requirement in test_node.resource_requirements.items():
                    current_resources[resource] = requirement
        
        if current_group:
            subgroups.append(current_group)
        
        return subgroups


class ParallelTestExecutor:
    """ä¸¦åˆ—ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.results: Dict[str, Any] = {}
        self.lock = threading.Lock()
        self.resource_locks: Dict[str, threading.Semaphore] = {}
    
    def execute_tests(self, 
                     test_groups: List[List[str]], 
                     test_runner: callable) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä¸¦åˆ—å®Ÿè¡Œ"""
        total_start_time = time.time()
        
        for group_index, test_group in enumerate(test_groups):
            print(f"\nğŸš€ ã‚°ãƒ«ãƒ¼ãƒ— {group_index + 1}/{len(test_groups)} ã‚’å®Ÿè¡Œä¸­...")
            print(f"   ãƒ†ã‚¹ãƒˆæ•°: {len(test_group)}")
            
            group_start_time = time.time()
            
            # ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®ãƒ†ã‚¹ãƒˆã‚’ä¸¦åˆ—å®Ÿè¡Œ
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {
                    executor.submit(self._run_test, test_id, test_runner): test_id
                    for test_id in test_group
                }
                
                for future in as_completed(futures):
                    test_id = futures[future]
                    try:
                        result = future.result()
                        with self.lock:
                            self.results[test_id] = result
                    except Exception as e:
                        with self.lock:
                            self.results[test_id] = {
                                "status": "error",
                                "error": str(e),
                                "duration": 0
                            }
            
            group_duration = time.time() - group_start_time
            print(f"   âœ… ã‚°ãƒ«ãƒ¼ãƒ—å®Œäº†: {group_duration:.2f}ç§’")
        
        total_duration = time.time() - total_start_time
        
        return {
            "results": self.results,
            "total_duration": total_duration,
            "groups_executed": len(test_groups),
            "tests_executed": len(self.results)
        }
    
    def _run_test(self, test_id: str, test_runner: callable) -> Dict[str, Any]:
        """å€‹åˆ¥ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        start_time = time.time()
        
        try:
            # ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
            test_runner(test_id)
            
            return {
                "status": "passed",
                "duration": time.time() - start_time
            }
            
        except Exception as e:
            return {
                "status": "failed",
                "error": str(e),
                "duration": time.time() - start_time
            }


class TestLoadBalancer:
    """ãƒ†ã‚¹ãƒˆã®è² è·åˆ†æ•£"""
    
    def __init__(self, historical_data_path: Optional[Path] = None):
        self.historical_data = {}
        if historical_data_path and historical_data_path.exists():
            with open(historical_data_path, 'r') as f:
                self.historical_data = json.load(f)
    
    def balance_tests(self, 
                     tests: List[TestNode], 
                     num_workers: int) -> List[List[TestNode]]:
        """ãƒ†ã‚¹ãƒˆã‚’ãƒ¯ãƒ¼ã‚«ãƒ¼é–“ã§å‡ç­‰ã«åˆ†æ•£"""
        # å®Ÿè¡Œæ™‚é–“ã®æ¨å®šå€¤ã§é™é †ã‚½ãƒ¼ãƒˆ
        sorted_tests = sorted(tests, key=lambda t: self._estimate_duration(t), reverse=True)
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ã”ã¨ã®è² è·ã‚’è¿½è·¡
        worker_loads = [0.0] * num_workers
        worker_tests = [[] for _ in range(num_workers)]
        
        # è²ªæ¬²æ³•ã§å‰²ã‚Šå½“ã¦
        for test in sorted_tests:
            # æœ€ã‚‚è² è·ã®å°‘ãªã„ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’é¸æŠ
            min_load_index = worker_loads.index(min(worker_loads))
            
            # ãƒ†ã‚¹ãƒˆã‚’å‰²ã‚Šå½“ã¦
            worker_tests[min_load_index].append(test)
            worker_loads[min_load_index] += self._estimate_duration(test)
        
        return worker_tests
    
    def _estimate_duration(self, test: TestNode) -> float:
        """ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œæ™‚é–“ã‚’æ¨å®š"""
        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ä½¿ç”¨
        if test.id in self.historical_data:
            historical_durations = self.historical_data[test.id].get("durations", [])
            if historical_durations:
                # æœ€è¿‘ã®å®Ÿè¡Œæ™‚é–“ã®å¹³å‡ã‚’ä½¿ç”¨
                return sum(historical_durations[-5:]) / len(historical_durations[-5:])
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ¨å®šå€¤ã‚’è¿”ã™
        return test.estimated_duration
    
    def update_historical_data(self, test_id: str, duration: float):
        """å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°"""
        if test_id not in self.historical_data:
            self.historical_data[test_id] = {"durations": []}
        
        self.historical_data[test_id]["durations"].append(duration)
        
        # æœ€æ–°ã®10ä»¶ã®ã¿ä¿æŒ
        self.historical_data[test_id]["durations"] = \
            self.historical_data[test_id]["durations"][-10:]
    
    def save_historical_data(self, path: Path):
        """å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        with open(path, 'w') as f:
            json.dump(self.historical_data, f, indent=2)


class SmartTestScheduler:
    """ã‚¹ãƒãƒ¼ãƒˆãªãƒ†ã‚¹ãƒˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼"""
    
    def __init__(self):
        self.dependency_graph = TestDependencyGraph()
        self.load_balancer = TestLoadBalancer()
        self.executor = ParallelTestExecutor()
    
    def analyze_test_suite(self, test_files: List[Path]) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’åˆ†æ"""
        analysis = {
            "total_tests": 0,
            "modules": set(),
            "dependencies_found": 0,
            "resource_intensive_tests": [],
            "long_running_tests": []
        }
        
        for test_file in test_files:
            # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æï¼ˆç°¡æ˜“ç‰ˆï¼‰
            module_name = test_file.stem
            analysis["modules"].add(module_name)
            
            # ãƒ†ã‚¹ãƒˆã‚’è¿½åŠ ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ AST è§£æãªã©ã‚’ä½¿ç”¨ï¼‰
            test_node = TestNode(
                name=f"test_{module_name}",
                module=module_name,
                estimated_duration=5.0,
                resource_requirements={"cpu": 0.25, "memory": 0.2}
            )
            
            self.dependency_graph.add_test(test_node)
            analysis["total_tests"] += 1
        
        return analysis
    
    def create_optimal_schedule(self, max_workers: int = 4) -> List[List[str]]:
        """æœ€é©ãªå®Ÿè¡Œã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½œæˆ"""
        # ä¾å­˜é–¢ä¿‚ã‚’è€ƒæ…®ã—ãŸå®Ÿè¡Œé †åºã‚’å–å¾—
        execution_groups = self.dependency_graph.optimize_for_resources(max_workers)
        
        # å„ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§è² è·åˆ†æ•£
        optimized_schedule = []
        for group in execution_groups:
            # ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®ãƒ†ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã‚’å–å¾—
            group_tests = [self.dependency_graph.test_nodes[test_id] for test_id in group]
            
            # è² è·åˆ†æ•£
            balanced_groups = self.load_balancer.balance_tests(group_tests, max_workers)
            
            # ãƒ†ã‚¹ãƒˆIDã®ãƒªã‚¹ãƒˆã«å¤‰æ›
            for worker_tests in balanced_groups:
                if worker_tests:
                    optimized_schedule.append([test.id for test in worker_tests])
        
        return optimized_schedule
    
    def estimate_execution_time(self, schedule: List[List[str]]) -> float:
        """å®Ÿè¡Œæ™‚é–“ã‚’æ¨å®š"""
        total_time = 0
        
        for group in schedule:
            # ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®æœ€å¤§å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
            group_times = []
            for test_id in group:
                test_node = self.dependency_graph.test_nodes.get(test_id)
                if test_node:
                    group_times.append(
                        self.load_balancer._estimate_duration(test_node)
                    )
            
            if group_times:
                total_time += max(group_times)
        
        return total_time


# ä½¿ç”¨ä¾‹
"""
# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®åˆæœŸåŒ–
scheduler = SmartTestScheduler()

# ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®åˆ†æ
test_files = list(Path("tests/e2e").glob("test_*.py"))
analysis = scheduler.analyze_test_suite(test_files)

# æœ€é©ãªã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½œæˆ
schedule = scheduler.create_optimal_schedule(max_workers=4)

# å®Ÿè¡Œæ™‚é–“ã®æ¨å®š
estimated_time = scheduler.estimate_execution_time(schedule)
print(f"æ¨å®šå®Ÿè¡Œæ™‚é–“: {estimated_time:.2f}ç§’")

# ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
def test_runner(test_id):
    # å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯
    pytest.main([test_id])

executor = ParallelTestExecutor(max_workers=4)
results = executor.execute_tests(schedule, test_runner)
"""